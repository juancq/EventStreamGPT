defaults:
  - finetune_config
  - _self_

do_overwrite: false
seed: 1
load_from_model_dir: null
task_df_name: ???
pretrained_weights_fp: null
config:
  do_use_learnable_sinusoidal_ATE: true
  do_split_embeddings: false
  static_embedding_mode: sum_all
  static_embedding_weight: 0.4
  dynamic_embedding_weight: 0.5
  do_normalize_by_measurement_index: false
  structured_event_processing_mode: conditionally_independent
  num_hidden_layers: 4
  seq_attention_types: ["global", "local"]
  seq_window_size: 4
  head_dim: 8
  num_attention_heads: 2
  attention_dropout: 0.2
  input_dropout: 0.2
  resid_dropout: 0.2
  intermediate_size: 128
  task_specific_params:
    pooling_method: mean
optimization_config:
  init_lr: 0.0001
  end_lr_frac_of_init_lr: 0.01
  end_lr: null
  max_epochs: 2
  batch_size: 32
  validation_batch_size: 32
  lr_frac_warmup_steps: 0.05
  lr_decay_power: 2
  weight_decay: 0.2
  patience: null
  gradient_accumulation: 2
  num_dataloader_workers: 1
data_config:
  save_dir: ???
  max_seq_len: 128
  min_seq_len: 4
trainer_config:
  accelerator: cpu
  devices: auto
  detect_anomaly: true
  log_every_n_steps: 1
experiment_dir: ???
wandb_logger_kwargs:
  name: null
  project: null
  team: null
  log_model: false
  do_log_graph: false
do_use_filesystem_sharing: false
